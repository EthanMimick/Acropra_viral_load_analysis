# after processing the contigs were binned by metabat2 maxbin2 and concoct before dereplication using drep.
conda activate binning
# The raw reads have to be copied into the SRR directories
for f in *_pass_1.fastq.gz;
 do
b${f%_pass_1.fastq.gz};
cp ${f} ./${b};
cp ${b} ./${b}_pass_2.fastq.gz;
done
# creating depth file for matabat2
for f in ./SRR*/*_pass_1.fastq.gz;
 do
b=${f%_pass_1.fastq.gz}; 
bwa mem -t 10 contigs.fasta ${f} ${b}_pass_2.fastq.gz > ${b}.sam;
samtools sort ${b}.sam -o ${b}_sorted.bam;
jgi_summarize_bam_contig_depths --outputDepth ${b}_depth.txt ${b}_sorted.bam;
done
# metabat2 
for f in ./SRR*/*_pass_1.fastq.gz;
do
b=${f%_pass_1.fastq.gz};
bwa index contigs.fasta;
metabat2 -i contigs.fasta -a ${b}_depth.txt -o bins/bin;
done
# maxbin2
for f in ./SRR*/*_pass_1.fastq.gz;
do
b=${f%_pass_1.fastq.gz};
cut -f1,4,6,8,10 ${b}_depth.txt > maxbin.txt:
run_MaxBin.pl -thread 10 -contig contigs.fasta -out bins/bin -abund maxbin.txt;
done
# concoct
for f in ./SRR*/*_pass_1.fastq.gz;
do
b=${f%_pass_1.fastq.gz};
samtools index ${b}_sorted.bam;
cut_up_fasta.py contigs.fasta -c 10000 -o 0 --merge_last -b contigs_10K.bed > contigs_10K.fa;
concoct -t 10 --composition_file contigs_10K.fa --coverage_file coverage_table.tsv -b concoct_output/;
merge_cutup_clustering.py concoct_output/clustering_gt1000.csv > concoct_output/clustering_merged.csv;
extract_fasta_bins.py contigs.fasta concoct_output/clustering_merged.csv --output_path bins/bin;
done
# drep
for f in ./SRR*/*_pass_1.fastq.gz;
do
b=${f%_pass_1.fastq.gz};
dRep dereplicate drep -d -g ./bins/*.fa;
done

